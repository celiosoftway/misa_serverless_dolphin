import os
import time
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import runpod

# Se vocÃª mantiver o symlink workspace -> runpod-volume:
# MODEL_PATH = "/workspace/misa-dolphin"
# Se estiver acessando direto o mountpoint:
MODEL_PATH = "/runpod-volume/misa-dolphin"

tokenizer = None
model = None


def wait_for_path(path: str):
    print(f"â³ Aguardando path existir: {path}")
    tries = 0
    while not os.path.exists(path):
        time.sleep(1)
        tries += 1
        if tries % 5 == 0:
            print(f"ğŸ“Œ Ainda aguardando path: {path} (tentativa {tries})")
    print(f"ğŸ“‚ Path disponÃ­vel: {path}")


def debug_list(path: str):
    print("ğŸ” Listando conteÃºdo do diretÃ³rio do modelo:")
    for root, dirs, files in os.walk(path):
        print(f"ğŸ“ DIR: {root}")
        for d in dirs:
            print(f"   ğŸ“‚ {d}/")
        for f in files:
            print(f"   ğŸ“„ {f}")
    print("ğŸ” Fim da listagem.\n")


def load_model():
    global tokenizer, model

    if tokenizer is not None and model is not None:
        return tokenizer, model

    # 1) Garante que o path existe
    wait_for_path(MODEL_PATH)

    # 2) Lista tudo que existe lÃ¡ dentro
    debug_list(MODEL_PATH)

    # 3) Carrega tokenizer e modelo sÃ³ de arquivo local
    print("ğŸš€ Carregando tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_PATH,
        local_files_only=True,
        trust_remote_code=True
    )

    print("ğŸš€ Carregando modelo...")
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH,
        local_files_only=True,
        trust_remote_code=True,
        torch_dtype=torch.float16,
        device_map="auto"
    )

    model.eval()
    print("âœ… Modelo carregado em modo de inferÃªncia.\n")

    return tokenizer, model


def handler(job):
    job_input = job.get("input", {})
    prompt = job_input.get("prompt")

    if not prompt:
        return {"error": "Campo 'prompt' Ã© obrigatÃ³rio"}

    print(f"ğŸ“ Prompt recebido: {prompt[:80]}...\n")

    tokenizer, model = load_model()

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    output_tokens = model.generate(
        **inputs,
        max_new_tokens=512,
        do_sample=True,
        temperature=0.7,
        repetition_penalty=1.1,
    )

    decoded = tokenizer.decode(output_tokens[0], skip_special_tokens=True)

    return {"output": decoded}


runpod.serverless.start({"handler": handler})
